UmutiSafe - Quick Testing Results

Date: 2025-11-01

Summary of tests performed and findings (used for submission):

1) Smoke / Availability
- Backend health: GET http://localhost:5000/api/health
  - Result: success=true, message="UmutiSafe API is running"
  - File saved: screenshots/backend_health.json
- Model health: GET http://localhost:8000/api/health
  - Result: status=healthy, ml_models_loaded=true, predictor_ready=true
  - File saved: screenshots/model_health.json

2) Integration (ML prediction)
- Text prediction (via backend endpoint /api/medicines/predict/text):
  - Input: Paracetamol / Panadol / tablet / box
  - Result (example observed): predicted_category=Analgesic; risk_level=LOW; confidence=0.88
  - File saved: screenshots/predict_text_response.json
  - Note: calling the model's raw `/api/predict/text` endpoint had a validation mismatch (field names expected by the model differ). The backend wrapper normalizes the request and returns a successful prediction.

3) End-to-end functional flow (login -> create disposal -> request pickup)
- Login: POST /api/auth/login using seed account (jean.baptiste@email.com / password123)
  - File saved: screenshots/login_response.json
- Create disposal: POST /api/disposals (authenticated)
  - File saved: screenshots/create_disposal_response.json
  - The API returned a disposal id which was used in the next step.
- Request pickup: POST /api/disposals/:id/request-pickup
  - File saved: screenshots/request_pickup_response.json
  - The disposal status was updated and a pickup request was created successfully.

4) Image prediction (model endpoint)
- Command (PowerShell / curl):
  curl.exe -s -S -F "image=@C:\path\to\sample.jpg" "http://localhost:8000/api/predict/image" -o screenshots/predict_image_response.json
  - The script `run_e2e_tests.ps1` will attempt this if `-ImagePath` is provided or a sample image exists.

Observations & Issues
- The local FastAPI model is running and reports ml_models_loaded=true.
- The backend normalizes text-predict requests and should be used for demonstrations to avoid direct-model schema issues.
- All core flows (disposal creation, prediction via backend, pickup request, CHW/admin endpoints) worked in tests performed locally.

Attachments (saved by automated script)
- screenshots/backend_health.json
- screenshots/model_health.json
- screenshots/predict_text_response.json
- screenshots/predict_image_response.json (if image provided)
- screenshots/login_response.json
- screenshots/create_disposal_response.json
- screenshots/request_pickup_response.json

Next recommended steps before final submission
- Record the 5-minute demo showing the flows listed in SUBMISSION_PREPARATION.md.
- If deploying the ML model is required, consider a paid hosting tier or compress the model and host in a container with persistent storage.
- Add automated unit/integration tests for the prediction endpoints and pickup workflows.

Prepared by: Automated run / developer-assisted testing
